<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <title>Challenge Description | ActivityNet Large Scale Activity Recognition Challenge 2018</title>
    <!-- core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css" integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="css/main.css">

    <link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:400,700" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Droid+Sans+Mono' rel='stylesheet' type='text/css'>
    <!--[if lt IE 9]>
    <script src="../../js/html5shiv.js"></script>
    <script src="../../js/respond.min.js"></script>
    <![endif]-->
    <link rel="shortcut icon" media="all" type="image/x-icon" href="../../images/favicon.png" >

    <!-- Tracking code -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-56208223-2', 'auto');
      ga('send', 'pageview');

    </script>
</head><!--/head-->

<body id="challenge" class="challenge-page normal-page">
  <nav class="navbar navbar-expand-lg navbar-light">
    <div class="container"> 
    <a class="navbar-brand" href="index.html">
      <img src="images/ChallengeLogo.svg" class="d-inline-black-align-top" height="45" width="auto">
    </a>

    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarContent" aria-controls="navbarContent" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarContent">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link" href="index.html">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="people.html">People</a>
        </li>
        <li class="nav-item active">
          <a class="nav-link" href="challenge.html">Challenge</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="program.html">Program</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="evaluation.html">Evaluation</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="contact.html">Contact</a>
        </li>
        <li class="nav-item">
          <a class="nav-link right-logo" href="http://cvpr2018.thecvf.com" target="_black">
            <img src="images/cvpr18logo.jpg" class="img-responsive cvpr-logo" height="45" width="auto">
          </a>
        </li>
      </ul>
    </div> <!-- collapse -->
    </div>
  </nav>

  <section class="normal-page-title challenge-title">
    <!-- <img class="title-banner" src="images/people_banner.svg" /> -->
    <div class="container">
      <div class="row">
        <div class="col-12 col-sm-12 col-md-12">
            <h1 class="title">Challenge Description</h1>
        </div>
      </div>
    </div>
  </section>

  <section class="guidelines">
    <div class="container">
      <div class="row">
        <div class="col-md-12 col-sm-12">
            <h2 class="section-title">Challenge Introduction</h2>

            <p class="para-text">
              We are proud to announce that this year the challenge will host seven diverse tasks which aim to push the limits of semantic visual understanding of videos as well as bridging visual content with human captions. Three out of the seven tasks in the challenge are based on the <a href="http://activity-net.org/">ActivityNet dataset</a>, which was introduced in CVPR 2015 and organized hierarchically in a semantic taxonomy. These tasks focus on trace evidence of activities in time in the form of proposals, class labels, and <a href="http://cs.stanford.edu/people/ranjaykrishna/densevid/" target="_blank">captions</a>.
            </p>
            <p class="para-text">
              In this installment of the challenge, we will host four guest tasks which enrich the understanding of visual information in videos. These tasks focus on complementary aspects of the activity recognition problem at large scale and involve challenging and recently compiled activity/action datasets, including <a href="https://deepmind.com/kinetics" target="_blank">Kinetics</a> (Google DeepMind), <a href="https://research.google.com/ava/" target="_blank">AVA</a> (Berkeley and Google), SoA (Facebook), and <a href="http://moments.csail.mit.edu/" target="_blank"> Moments in Time </a> (MIT and IBM Research).
            </p>

<!--             <p>
              To enter the competition, you need to create an account on the <a href="evaluation.html"> Evaluation</a>.
              Using a registered account you will be able to upload your results to the evaluation server and participate in the ActivityNet Challenge 2017. <strong class="line-through">Please be advised that we have changed our submission policy this year: each participant is limited to 1 submission per task per week. The Evaluation Server will enforce a waiting time of 7 days between submissions of the same task. This gives each participant a total of 3 submissions per task before the evaluation server closes.</strong> <strong> Each team is limited to 4 submissions (in total) per task.</strong> Only results that are submitted during the challenge period (before the deadline) and posted to the leaderboard will be considered valid. Additionally, you will also need to upload a notebook paper that describes your method in detail. This challenge allows the use of external data to train and tune algorithm parameters. We are committed to keeping track of this practice. Therefore, each submission must explicitly cite the kind of external data used and which modules benefit from it.
            </p> -->

        </div>
      </div>
    </div>
  </section>

  <section class="tasks flex-list" id="activitynet-tasks">
    <div class="container">
      <div class="row">
        <div class="col-md-12 col-sm-12">
            <h2 class="section-title">ActivityNet Tasks</h2>

            <!-- Task 1 -->
            <div class="row">
              <div class="col-12">
                <div class="flex-list-item">
                  <a href="tasks/anet_proposals.html">
                  <div class="flex-item-icon">
                    <div class="image">
                      <img class="img-responsive" src="images/icon/anet_proposals.png"/>
                    </div>
                    <div class="text">
                      <h4 class="has-label">Task 1</h4>
                      <div class="label">2nd trial</div>
                    </div>
                  </div>
                  </a>
                  <div class="flex-item-info">
                    <h4 class="name">Temporal Action Proposals (ActivityNet)</h4>
                    <p class="description">
                      This task is intended to evaluate the ability of algorithms to generate high quality <strong>action proposals</strong>. The goal is to produce a set of candidate temporal segments that are likely to contain a human action. 
                    </p>
                    <div class="flex-item-button">
                      <a href="tasks/anet_proposals.html" class="box-animated-link">
                        <span>Details</span>
                      </a>
                    </div>
                  </div>                  
                </div>
              </div>
            </div>

            <!-- Task 2 -->
            <div class="row">
              <div class="col-12">
                <div class="flex-list-item">
                  <a href="tasks/anet_localization.html">
                  <div class="flex-item-icon">
                    <div class="image">
                      <img class="img-responsive" src="images/icon/anet_localization.png"/>
                    </div>
                    <div class="text">
                      <h4 class="has-label">Task 2</h4>
                      <div class="label">3rd trial</div>
                    </div>
                  </div>
                  </a>
                  <div class="flex-item-info">
                    <h4 class="name">Temporal Action Localization (ActivityNet)</h4>
                    <p class="description">
                      This task is intended to evaluate the ability of algorithms to <strong>temporally localize activities in untrimmed video sequences</strong>. Here, videos can contain more than one activity instance, and mutiple activity categories can appear in the video.
                    </p>
                    <div class="flex-item-button">
                      <a href="tasks/anet_localization.html" class="box-animated-link">
                        <span>Details</span>
                      </a>
                    </div>
                  </div>
                </div>
              </div>
            </div>

            <!-- Task 3 -->
            <div class="row">
              <div class="col-12">
                <div class="flex-list-item">
                  <a href="tasks/anet_captioning.html">
                  <div class="flex-item-icon">
                    <div class="image">
                      <img class="img-responsive" src="images/icon/anet_captioning.png"/>
                    </div>
                    <div class="text">
                      <h4 class="has-label">Task 3</h4>
                      <div class="label">2nd trial</div>
                    </div>
                  </div>
                  </a>
                  <div class="flex-item-info">
                    <h4 class="name">Dense-Captioning Events in Videos (ActivityNet Captions)</h4>
                    <p class="description">
                      This task involves both <strong>detecting and describing events</strong> in a video. For this task, participants will use the ActivityNet Captions dataset, a new large-scale benchmark for dense-captioning events.
                    </p>
                    <div class="flex-item-button">
                      <a href="tasks/anet_captioning.html" class="box-animated-link">
                        <span>Details</span>
                      </a>
                    </div>
                  </div>
                </div>
              </div>
            </div>
        </div>
      </div>
    </div>
  </section>

  <section class="tasks flex-list" id="guest-tasks">
    <div class="container">
      <div class="row">
        <div class="col-md-12 col-sm-12">

            <h2 class="section-title">Guest Tasks</h2>

            <!-- Task A: Kinetics -->
            <div class="row">
              <div class="col-12">
                <div class="flex-list-item">
                  <div class="flex-item-icon">
                    <div class="image">
                      <img class="img-responsive" src="images/icon/kinetics.png"/>
                    </div>
                    <div class="text">
                      <h4 class="has-label">Task A</h4>
                      <div class="label">2nd trial</div>
                    </div>
                  </div>
                  <div class="flex-item-info">
                    <h4 class="name">Trimmed Activity Recognition (Kinetics)</h4>
                    <p class="description">
                      This task is intended to evaluate the ability of algorithms to recognize activities in <strong>trimmed</strong> video sequences. Here, videos contain a single activity, and all the clips have a standard duration of ten seconds. For this task, participants will use the Kinetics dataset, a large-scale benchmark for trimmed action classification.
                    </p>
                    <div class="flex-item-button">
                      <a href="#guest-tasks" class="box-animated-link">
                        <span>Details Soon</span>
                      </a>
                    </div>
                  </div>
                </div>
              </div>
            </div>

            <!-- Task B: AVA -->
            <div class="row">
              <div class="col-12">
                <div class="flex-list-item">
                  <a href="tasks/guest_ava.html">
                  <div class="flex-item-icon">
                    <div class="image">
                      <img class="img-responsive" src="images/icon/ava.png"/>
                    </div>
                    <div class="text">
                      <h4 class="has-label">Task B</h4>
                      <div class="label">New</div>
                    </div>
                  </div>
                  </a>
                  <div class="flex-item-info">
                    <h4 class="name">Spatio-temporal Action Localization (AVA)</h4>
                    <p class="description">
                      This task is intended to evaluate the ability of algorithms to <b>localize human
                      actions in space and time.</b> Each labeled video segment can contain multiple
                      subjects, each performing potentially multiple actions. The goal is to identify
                      these subjects and actions over continuous 15-minute video clips extracted from
                      movies. For this task, participants will use the new AVA atomic visual actions
                      dataset.
                    </p>
                    <div class="flex-item-button">
                      <a href="tasks/guest_ava.html" class="box-animated-link">
                        <span>Details</span>
                      </a>
                    </div>
                  </div>
                </div>
              </div>
            </div>

            <!-- Task C: SOA -->
            <div class="row">
              <div class="col-12">
                <div class="flex-list-item">
                  <a href="tasks/guest_soa.html">
                  <div class="flex-item-icon">
                    <div class="image">
                      <img class="img-responsive" src="images/icon/soa.png"/>
                    </div>
                    <div class="text">
                      <h4 class="has-label">Task C</h4>
                      <div class="label">New</div>
                    </div>
                  </div>
                  </a>
                  <div class="flex-item-info">
                    <h4 class="name">Multi-label, multi-task video classification (SOA)</h4>
                    <p class="description">
                    This task is intended to evaluate the ability of algorithms to classify short video clips (about 10 seconds). Each video is densely annotated with <b>multiple labels covering scene, object and action.</b> For this task, participants will use the SOA dataset, a new large-scale benchmark for multi-label and multi-task video classification.
                    </p>
                    <div class="flex-item-button">
                      <a href="tasks/guest_soa.html" class="box-animated-link">
                        <span>Details</span>
                      </a>
                    </div>
                  </div>
                </div>
              </div>
            </div>

            <!-- Task D: Moments -->
            <div class="row">
              <div class="col-12">
                <div class="flex-list-item">
                  <a href=href="http://moments.csail.mit.edu/challenge.html" target="_blank">
                  <div class="flex-item-icon">
                    <div class="image">
                      <img class="img-responsive" src="images/icon/moments.png"/>
                    </div>
                    <div class="text">
                      <h4 class="has-label">Task D</h4>
                      <div class="label">New</div>
                    </div>
                  </div>
                  </a>
                  <div class="flex-item-info">
                    <h4 class="name">Trimmed Event Recognition (Moments in Time)</h4>
                    <p class="description">
                    This task is intended to evaluate the ability of algorithms to <b>classify events in trimmed 3-second videos.</b> Here, videos contain a single activity, and all clips have a standard duration of 3 seconds. There will be two tracks. The first track will use the Moments in Time dataset, a new large-scale dataset for video understanding, which has 800K videos in the training set. For the second track, participants will use the Moments in Time Mini dataset, a subset of Moments in Time with 100k videos provided in the training set.
                    </p>
                    <div class="flex-item-button">
                      <a href="http://moments.csail.mit.edu/challenge.html" class="box-animated-link" target="_blank">
                        <span>Details</span>
                      </a>
                    </div>
                  </div>
                </div>
              </div>
            </div>

        </div>
      </div>
    </div>
  </section>


  <footer class="footer">
    <!-- <img src="images/footer_pattern.svg" alt="" class="footer-pattern"> -->
    <div class="footer-content">
      <div class="container">
        <div class="row">
          <div class="col-md-4 col-xs-12">
            <img src="images/ChallengeLogo_White.svg" alt="" class="footer-logo">
          </div>
          <div class="col-md-4 col-xs-12">
            <ul class="footer-nav">
              <li class="nav-item">
                <a href="index.html">Home</a>
              </li>
              <li class="nav-item">
                <a href="people.html">People</a>
              </li>
              <li class="nav-item">
                <a href="challenge.html">Challenge</a>
              </li>
              <li class="nav-item">
                <a href="program.html">Program</a>
              </li>
              <li class="nav-item">
                <a href="evaluation.html">Evaluation</a>
              </li>
            </ul>
          </div>
          <div class="col-md-4 col-xs-12">
            <div class="footer-text-section">
              <h4 class="footer-section-title">Contact</h4>
              <p class="footer-section-text">
                For general information or inquiry about the ActivityNet workshop (evaluation server, dates, or program), please contact <strong>Fabian Caba </strong> <a href="mailto:fabian.caba@kaust.edu.sa?Subject=ActivityNet Challenge Inquiry" target="_top">fabian.caba@kaust.edu.sa</a>
              </p>
            </div>
            <div class="footer-text-section">
              <h4 class="footer-section-title">FAQ</h4>
              <p class="footer-section-text">
                For ActivityNet Database FAQs visit our<a href="https://groups.google.com/forum/#!forum/activity-net" target="_blank"> Google Group.</a>
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </footer>
  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.3/umd/popper.min.js" integrity="sha384-vFJXuSJphROIrBnz7yo7oB41mKfc8JzQZiCq4NCceLEaO4IHwicKwpJf9c9IpFgh" crossorigin="anonymous"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/js/bootstrap.min.js" integrity="sha384-alpBpkh1PFOepccYVYDB4do5UnbKysX5WZXm3XxPqe5iKTfUKjNkCk9SaVuEZflJ" crossorigin="anonymous"></script>

</body>
</html>
